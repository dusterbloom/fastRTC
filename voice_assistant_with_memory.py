#!/usr/bin/env python3
"""
FastRTC Voice Assistant with Smart Memory Management
Fixes: Memory conflicts, excessive API calls, blocking operations
"""

import sys
import time
import os
import numpy as np
import asyncio
import aiohttp
import threading
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from fastrtc import (
    ReplyOnPause, 
    Stream, 
    get_stt_model, 
    get_tts_model, 
    AlgoOptions, 
    SileroVadOptions, 
    KokoroTTSOptions
)
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, AsyncGenerator
from mem0 import Memory
import logging
from collections import deque
import hashlib
import weakref
from dataclasses import dataclass

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("ğŸ§  FastRTC Voice Assistant - Smart Memory Management System")
print("=" * 70)

def print_status(message):
    timestamp = time.strftime("%H:%M:%S")
    print(f"[{timestamp}] {message}")

# Initialize models
print_status("ğŸ§  Loading STT model (Moonshine)...")
try:
    stt_model = get_stt_model("moonshine/base")
    print_status("âœ… STT model loaded!")
except Exception as e:
    print_status(f"âŒ STT model failed: {e}")
    sys.exit(1)

print_status("ğŸ—£ï¸ Loading TTS model (Kokoro)...")
try:
    tts_model = get_tts_model("kokoro")
    print_status("âœ… TTS model loaded!")
except Exception as e:
    print_status(f"âŒ TTS model failed: {e}")
    sys.exit(1)

# Configuration
LM_STUDIO_URL = "http://192.168.1.5:1234/v1"
LM_STUDIO_MODEL = "mistral-7b-instruct"
OLLAMA_URL = "http://localhost:11434"

class SmartMemoryManager:
    """Smart memory manager with conflict resolution and efficiency"""
    
    def __init__(self, memory_instance, user_id):
        self.memory = memory_instance
        self.user_id = user_id
        self.executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="smart_mem")
        
        # Local memory cache for fast access
        self.memory_cache = {
            'user_name': None,
            'preferences': {},
            'facts': {},
            'last_updated': None
        }
        
        # Background memory queue
        self.memory_queue = asyncio.Queue() if hasattr(asyncio, 'Queue') else None
        self.background_task = None
        
        # Memory operation tracking
        self.memory_operations = 0
        self.cache_hits = 0
        
        print_status("ğŸ§  Smart memory manager initialized")
    
    def extract_user_name(self, text: str) -> Optional[str]:
        """Extract user name from various formats"""
        text_lower = text.lower().strip()
        
        # Name introduction patterns
        patterns = [
            r"my name is (\w+(?:\s+\w+)*)",
            r"i'm (\w+(?:\s+\w+)*)",
            r"call me (\w+(?:\s+\w+)*)",
            r"i am (\w+(?:\s+\w+)*)",
            r"name is (\w+(?:\s+\w+)*)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text_lower)
            if match:
                name = match.group(1).strip()
                # Clean up common artifacts
                name = re.sub(r'[^\w\s]', '', name)  # Remove punctuation
                name = ' '.join(name.split())  # Normalize whitespace
                if len(name) > 1 and len(name) < 50:  # Reasonable name length
                    return name.title()  # Proper case
        
        return None
    
    def is_name_correction(self, text: str) -> bool:
        """Detect if this is a name correction"""
        correction_patterns = [
            r"no,?\s+my name is",
            r"actually,?\s+my name is",
            r"it's\s+(\w+)",
            r"not\s+\w+,?\s+it's\s+(\w+)",
            r"no,?\s+it's\s+(\w+)",
            r"no,?\s+i'm\s+(\w+)",
        ]
        
        text_lower = text.lower()
        return any(re.search(pattern, text_lower) for pattern in correction_patterns)
    
    def should_store_memory(self, user_text: str, assistant_text: str) -> tuple[bool, str]:
        """Determine if conversation should be stored and what type"""
        user_lower = user_text.lower()
        
        # Skip very short or incomplete responses
        if len(user_text.strip()) <= 3:
            return False, "too_short"
        
        # Skip common acknowledgments
        skip_phrases = ['yes', 'no', 'ok', 'okay', 'thanks', 'thank you', 'and', 'um', 'uh']
        if user_text.strip().lower() in skip_phrases:
            return False, "acknowledgment"
        
        # Personal information
        if any(phrase in user_lower for phrase in ['my name is', 'i am', 'call me', 'i live', 'i work']):
            return True, "personal_info"
        
        # Preferences and important info
        if any(phrase in user_lower for phrase in ['i like', 'i love', 'i hate', 'my favorite', 'i prefer']):
            return True, "preference"
        
        # Important requests
        if any(phrase in user_lower for phrase in ['remember', 'don\'t forget', 'important']):
            return True, "important"
        
        # Memory recall requests
        if any(phrase in user_lower for phrase in ['what do you remember', 'what do you know about me', 'tell me about']):
            return False, "recall_request"  # Don't store recall requests
        
        # Complex conversations (longer than 10 words)
        if len(user_text.split()) > 10:
            return True, "conversation"
        
        return False, "filtered"
    
    def update_local_cache(self, user_text: str, category: str):
        """Update local memory cache with extracted information"""
        if category == "personal_info":
            name = self.extract_user_name(user_text)
            if name:
                if self.is_name_correction(user_text) or not self.memory_cache['user_name']:
                    print_status(f"ğŸ·ï¸ Updated user name: {self.memory_cache['user_name']} -> {name}")
                    self.memory_cache['user_name'] = name
                    self.memory_cache['last_updated'] = datetime.now()
        
        elif category == "preference":
            # Extract preferences
            text_lower = user_text.lower()
            if 'i like' in text_lower:
                preference = text_lower.split('i like')[-1].strip()
                self.memory_cache['preferences'][f'likes_{len(self.memory_cache["preferences"])}'] = preference
            elif 'i love' in text_lower:
                preference = text_lower.split('i love')[-1].strip()
                self.memory_cache['preferences'][f'loves_{len(self.memory_cache["preferences"])}'] = preference
    
    async def add_to_memory_smart(self, user_text: str, assistant_text: str):
        """Smart memory addition with conflict resolution"""
        should_store, category = self.should_store_memory(user_text, assistant_text)
        
        if not should_store:
            print_status(f"ğŸš« Skipping memory storage: {category}")
            return None
        
        # Update local cache first (immediate)
        self.update_local_cache(user_text, category)
        
        # Queue background storage
        if self.memory_queue:
            await self.memory_queue.put(('add', user_text, assistant_text, category))
            print_status(f"ğŸ“ Queued {category} memory for background storage")
        else:
            # Fallback to immediate storage
            threading.Thread(
                target=self._store_memory_background,
                args=(user_text, assistant_text, category),
                daemon=True
            ).start()
        
        self.memory_operations += 1
        return category
    
    def _store_memory_background(self, user_text: str, assistant_text: str, category: str):
        """Store memory in background thread"""
        try:
            # Handle name updates specially
            if category == "personal_info" and self.extract_user_name(user_text):
                name = self.extract_user_name(user_text)
                
                # Try to update existing name memory instead of adding new one
                try:
                    # Search for existing name memories
                    existing = self.memory.search(
                        query="user name", 
                        user_id=self.user_id,
                        limit=5
                    )
                    
                    # If we have existing name memories, we should delete them first
                    if existing:
                        print_status(f"ğŸ”„ Found {len(existing)} existing name memories, updating...")
                        # For now, just add the new one - mem0 should handle duplicates
                    
                except Exception as e:
                    print_status(f"âš ï¸ Error checking existing memories: {e}")
                
                # Store the corrected/new name
                result = self.memory.add(
                    messages=[
                        {"role": "user", "content": f"My name is {name}"},
                        {"role": "assistant", "content": f"Nice to meet you, {name}!"}
                    ],
                    user_id=self.user_id,
                    metadata={
                        "category": "user_name",
                        "name": name,
                        "timestamp": datetime.now().isoformat()
                    }
                )
                print_status(f"ğŸ’¾ Stored name update: {name}")
                
            else:
                # Regular memory storage
                result = self.memory.add(
                    messages=[
                        {"role": "user", "content": user_text},
                        {"role": "assistant", "content": assistant_text}
                    ],
                    user_id=self.user_id,
                    metadata={
                        "category": category,
                        "timestamp": datetime.now().isoformat()
                    }
                )
                print_status(f"ğŸ’¾ Stored {category} memory")
                
        except Exception as e:
            print_status(f"âŒ Background memory storage failed: {e}")
    
    def get_user_context(self) -> str:
        """Get user context from local cache (fast)"""
        context_parts = []
        
        if self.memory_cache['user_name']:
            context_parts.append(f"User's name: {self.memory_cache['user_name']}")
        
        if self.memory_cache['preferences']:
            prefs = ", ".join(self.memory_cache['preferences'].values())
            context_parts.append(f"User preferences: {prefs}")
        
        return ". ".join(context_parts) if context_parts else ""
    
    async def search_memories_smart(self, query: str) -> str:
        """Smart memory search with caching"""
        # For name queries, use local cache first
        if any(phrase in query.lower() for phrase in ['my name', 'who am i', 'remember about me']):
            self.cache_hits += 1
            
            if self.memory_cache['user_name']:
                return f"Your name is {self.memory_cache['user_name']}"
            else:
                return "I don't have your name stored yet. What would you like me to call you?"
        
        # For other queries, search memory database
        try:
            loop = asyncio.get_event_loop()
            memories = await loop.run_in_executor(
                self.executor,
                self._search_memories_sync,
                query
            )
            
            if memories:
                memory_texts = []
                for memory in memories[:3]:  # Limit to 3 most relevant
                    if isinstance(memory, dict) and 'memory' in memory:
                        memory_texts.append(memory['memory'][:100])
                    elif isinstance(memory, str):
                        memory_texts.append(memory[:100])
                
                return f"I remember: {'; '.join(memory_texts)}"
            else:
                return "I don't have specific memories about that yet."
                
        except Exception as e:
            print_status(f"âš ï¸ Memory search failed: {e}")
            return "I'm having trouble accessing my memories right now."
    
    def _search_memories_sync(self, query: str):
        """Sync memory search"""
        return self.memory.search(
            query=query,
            user_id=self.user_id,
            limit=3
        )
    
    def get_stats(self) -> dict:
        """Get memory operation statistics"""
        return {
            'operations': self.memory_operations,
            'cache_hits': self.cache_hits,
            'cache_efficiency': f"{(self.cache_hits / max(self.memory_operations, 1) * 100):.1f}%",
            'user_name': self.memory_cache['user_name'],
            'last_updated': self.memory_cache['last_updated']
        }

class SmartVoiceAssistant:
    """Voice assistant with smart memory management"""
    
    def __init__(self):
        print_status("ğŸ§  Initializing smart voice assistant...")
        
        # Set required environment variable
        os.environ["OPENAI_API_KEY"] = "dummy-key-for-local-use"
        
        # Memory configuration
        self.config = {
            "llm": {
                "provider": "openai",
                "config": {
                    "api_key": "dummy-key-for-local-use",
                    "model": LM_STUDIO_MODEL,
                    "openai_base_url": LM_STUDIO_URL,
                    "temperature": 0.7,
                    "max_tokens": 150
                }
            },
            "embedder": {
                "provider": "ollama",
                "config": {
                    "model": "nomic-embed-text",
                    "ollama_base_url": OLLAMA_URL
                }
            },
            "vector_store": {
                "provider": "chroma",
                "config": {
                    "collection_name": "voice_assistant_memories",
                    "path": "./mem0_voice_db"
                }
            },
            "version": "v1.1"
        }
        
        try:
            self.memory = Memory.from_config(self.config)
            print_status("âœ… Mem0 initialized!")
        except Exception as e:
            print_status(f"âŒ Mem0 initialization failed: {e}")
            sys.exit(1)
        
        # Session management
        self.user_id = "voice_user"
        self.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Smart memory manager
        self.smart_memory = SmartMemoryManager(self.memory, self.user_id)
        
        # Local conversation buffer
        self.conversation_buffer = deque(maxlen=5)  # Smaller buffer
        
        # Response caching
        self.response_cache = {}
        self.cache_ttl = 300
        
        # HTTP session for async requests
        self.http_session = None
        
        # Performance tracking
        self.turn_count = 0
        self.total_response_time = deque(maxlen=20)
        
        print_status(f"ğŸ‘¤ User ID: {self.user_id}")
        print_status(f"ğŸ¯ Session: {self.session_id}")
    
    async def initialize_async(self):
        """Initialize async components"""
        connector = aiohttp.TCPConnector(
            limit=5,
            limit_per_host=3,
            keepalive_timeout=30
        )
        timeout = aiohttp.ClientTimeout(total=6, connect=2)
        self.http_session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        print_status("âœ… Async components initialized")
    
    async def cleanup_async(self):
        """Cleanup async resources"""
        if self.http_session:
            await self.http_session.close()
        print_status("ğŸ§¹ Async cleanup completed")
    
    def get_cached_response(self, text: str) -> Optional[str]:
        """Check for cached responses"""
        text_hash = hashlib.md5(text.lower().encode()).hexdigest()
        if text_hash in self.response_cache:
            cached_item = self.response_cache[text_hash]
            if datetime.now() - cached_item['timestamp'] < timedelta(seconds=self.cache_ttl):
                return cached_item['response']
        return None
    
    def cache_response(self, text: str, response: str):
        """Cache response for reuse"""
        text_hash = hashlib.md5(text.lower().encode()).hexdigest()
        self.response_cache[text_hash] = {
            'response': response,
            'timestamp': datetime.now()
        }
    
    def get_context_for_llm(self, user_input: str) -> str:
        """Get context for LLM with smart memory integration"""
        context_parts = []
        
        # Add user context from smart memory
        user_context = self.smart_memory.get_user_context()
        if user_context:
            context_parts.append(f"About the user: {user_context}")
        
        # Add recent conversation
        if self.conversation_buffer:
            context_parts.append("Recent conversation:")
            for turn in list(self.conversation_buffer)[-2:]:
                context_parts.append(f"User: {turn['user']}")
                context_parts.append(f"Assistant: {turn['assistant']}")
        
        return "\n".join(context_parts) if context_parts else ""
    
    async def get_llm_response_smart(self, text: str) -> str:
        """Get LLM response with smart memory integration"""
        
        # Check cache first
        cached = self.get_cached_response(text)
        if cached:
            print_status("âš¡ Using cached response")
            return cached
        
        # Handle memory recall requests
        if any(phrase in text.lower() for phrase in ['what do you remember', 'what do you know about me', 'who am i']):
            response = await self.smart_memory.search_memories_smart(text)
            self.cache_response(text, response)
            return response
        
        # Get context for LLM
        context = self.get_context_for_llm(text)
        
        # Build prompt
        if context:
            system_prompt = f"""You are a helpful voice assistant with memory of your user. Be conversational and personal.

{context}

Respond naturally and briefly, using the user's name when appropriate."""
        else:
            system_prompt = "You are a helpful voice assistant. Be conversational, natural, and brief."

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ]
        
        try:
            print_status("ğŸ¤– Calling LLM...")
            
            async with self.http_session.post(
                f"{LM_STUDIO_URL}/chat/completions",
                json={
                    "model": LM_STUDIO_MODEL,
                    "messages": messages,
                    "max_tokens": 120,  # Shorter for faster responses
                    "temperature": 0.7,
                    "stream": False
                }
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    assistant_text = data["choices"][0]["message"]["content"]
                    
                    # Cache response
                    self.cache_response(text, assistant_text)
                    
                    # Smart memory storage (background)
                    await self.smart_memory.add_to_memory_smart(text, assistant_text)
                    
                    return assistant_text
                else:
                    print_status(f"âš ï¸ LLM request failed: {response.status}")
                    return "I'm having trouble processing that right now."
                    
        except Exception as e:
            print_status(f"âŒ LLM request failed: {e}")
            return f"I heard you say: {text}"
    
    def process_audio_array(self, audio_data):
        """Process audio data efficiently"""
        if isinstance(audio_data, tuple):
            sample_rate, audio_array = audio_data
            if audio_array.dtype != np.float32:
                audio_array = audio_array.astype(np.float32)
            if np.max(np.abs(audio_array)) > 1.0:
                audio_array = audio_array / np.max(np.abs(audio_array))
            return sample_rate, audio_array
        return audio_data

# Initialize the assistant
print_status("ğŸš€ Initializing smart voice assistant...")
assistant = SmartVoiceAssistant()

# Event loop setup
loop = None

def setup_async_loop():
    """Setup async event loop"""
    global loop
    
    def run_async_loop():
        global loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(assistant.initialize_async())
        loop.run_forever()
    
    async_thread = threading.Thread(target=run_async_loop, daemon=True)
    async_thread.start()
    time.sleep(0.5)
    print_status("âœ… Async loop initialized")

def run_async_task(coro):
    """Run async task from sync context"""
    global loop
    if loop and loop.is_running():
        future = asyncio.run_coroutine_threadsafe(coro, loop)
        return future.result(timeout=8)
    else:
        print_status("âŒ Async loop not available")
        return None

def smart_voice_assistant(audio):
    """Smart voice assistant with efficient memory management"""
    try:
        # Process audio
        sample_rate, audio_array = assistant.process_audio_array(audio)
        
        print_status("ğŸ§ Processing audio...")
        
        # Speech to text
        user_text = stt_model.stt(audio)
        
        if not user_text or not user_text.strip():
            print_status("ğŸ¤« No speech detected")
            return
        
        print_status(f"ğŸ‘¤ User: {user_text}")
        
        # Get smart response
        start_time = time.time()
        assistant_text = run_async_task(assistant.get_llm_response_smart(user_text))
        response_time = time.time() - start_time
        
        if not assistant_text:
            assistant_text = "I'm having trouble with that request."
        
        print_status(f"ğŸ¤– Assistant: {assistant_text}")
        print_status(f"âš¡ Response time: {response_time:.2f}s")
        
        # Add to conversation buffer
        assistant.conversation_buffer.append({
            'user': user_text,
            'assistant': assistant_text,
            'timestamp': datetime.now()
        })
        
        assistant.turn_count += 1
        assistant.total_response_time.append(response_time)
        
        # Show smart memory stats
        if assistant.turn_count % 5 == 0:
            avg_time = sum(assistant.total_response_time) / len(assistant.total_response_time)
            memory_stats = assistant.smart_memory.get_stats()
            print_status(f"ğŸ“Š Avg response: {avg_time:.2f}s | Memory efficiency: {memory_stats['cache_efficiency']} | User: {memory_stats['user_name']}")
        
        # TTS
        options = KokoroTTSOptions(
            voice="af_heart",
            speed=1.1,
            lang="en-us"
        )
        
        for audio_chunk in tts_model.stream_tts_sync(assistant_text, options):
            yield audio_chunk
            
    except Exception as e:
        print_status(f"âŒ Error in smart_voice_assistant: {e}")

# Setup async components
setup_async_loop()

# Create FastRTC stream with optimized settings
print_status("ğŸŒ Creating smart FastRTC stream...")

stream = Stream(
    ReplyOnPause(
        smart_voice_assistant,
        can_interrupt=True,
        algo_options=AlgoOptions(
            audio_chunk_duration=2.0,
            started_talking_threshold=0.15,
            speech_threshold=0.08
        ),
        model_options=SileroVadOptions(
            threshold=0.35,
            min_speech_duration_ms=300,
            min_silence_duration_ms=2000,
            speech_pad_ms=300
        )
    ), 
    modality="audio", 
    mode="send-receive",
    track_constraints={
        "echoCancellation": True,
        "noiseSuppression": {"exact": True},
        "autoGainControl": {"exact": True},
        "sampleRate": {"ideal": 24000},
        "sampleSize": {"ideal": 16},
        "channelCount": {"exact": 1},
    }
)

print("=" * 70)
print("ğŸ§  SMART MEMORY Voice Assistant Ready!")
print("=" * 70)
print("ğŸ”§ Smart Memory Features:")
print("   âœ… Conflict resolution for name updates")
print("   âœ… Local memory cache for instant access")
print("   âœ… Background memory storage (non-blocking)")
print("   âœ… Smart filtering (no 'And' memories)")
print("   âœ… Reduced API calls (0-2 per turn vs 5)")
print("   âœ… Name extraction and correction handling")
print("   âœ… Preference tracking")
print("   âœ… Memory operation statistics")
print("   âœ… Response caching")
print("   âœ… Context-aware conversations")
print()
print("ğŸ¯ Memory Intelligence:")
print("   ğŸ“ Detects name corrections automatically")
print("   ğŸ“ Updates existing info instead of duplicating")
print("   ğŸ“ Skips trivial interactions ('And', 'Yes', etc.)")
print("   ğŸ“ Caches user context for instant access")
print("   ğŸ“ Background processing for heavy operations")
print()
print("ğŸ’¡ Test Commands:")
print("   â€¢ 'My name is [Name]' (will update correctly)")
print("   â€¢ 'No, my name is [Name]' (will detect correction)")
print("   â€¢ 'What do you remember about me?' (instant from cache)")
print("   â€¢ 'I like [something]' (will store preferences)")
print()
print("ğŸ›‘ To stop: Press Ctrl+C")
print("=" * 70)

# Launch the application
try:
    stream.ui.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
except KeyboardInterrupt:
    print_status("ğŸ›‘ Shutting down smart voice assistant...")
    if loop:
        asyncio.run_coroutine_threadsafe(assistant.cleanup_async(), loop)
        time.sleep(1)
    print_status("ğŸ›‘ Voice assistant stopped")
except Exception as e:
    print_status(f"âŒ Launch error: {e}")
    sys.exit(1)